# 如果没有指定group，append设定为yes，那么会添加到用户同名组；
# append设定为no，那么会添加到user组。
# 如果指定了group，那么都会添加在指定的group组
# 查看用户：cat /etc/passwd
# 查看组：cat /etc/group
- name: 添加 hadoop 用户 
  user: name={{ hadoop_user }} shell=/bin/bash password={{ hadoop_user_password }} append=yes
  when: add_user is defined

- name: 检查是否有生成用户的ssh公钥
  shell: "ls {{ hadoop_user_path }}/.ssh|grep '.pub' |wc -l"
  register: key_exist
  ignore_errors: true

- name: 生成ssh公钥
  user:
    name: "{{ hadoop_user }}"
    generate_ssh_key: yes
    ssh_key_bits: 2048
    ssh_key_file: .ssh/id_rsa
  when: "key_exist.stdout == '0'"
  
- name: 获取 Hadoop 用户公钥
  fetch: 
    src: "{{ hadoop_user_path }}/.ssh/id_rsa.pub"
    dest: "/tmp/id_{{ ansible_host }}_{{ hadoop_user }}.pub"
    flat: yes

# 从本地 authorized_keys 文件读取公钥内容
- name: 获取 master 的 Hadoop 用户公钥完成免密登陆 master
  authorized_key: 
    user: "{{ hadoop_user }}"
    key: "{{ lookup('file', '/tmp/id_{{ hadoop_master_ip }}_{{ hadoop_user }}.pub') }}"
    
- name: 安装 JDK
  yum: name={{ item }} state=present
  with_items: 
  - "java-1.8.0-openjdk"
  - "java-1.8.0-openjdk-devel"

- name: 配置 JAVA_HOME
  template: src=java_home.sh dest=/etc/profile.d

- name: 生效 JAVA_HOME
  shell: "source /etc/profile.d/java_home.sh"
  
- name: 准备 Hadoop 工作目录
  file: name={{ item }} state=directory owner={{ hadoop_user }} group={{ hadoop_group }} mode=0755
  with_items:
  - "{{ hadoop_work_path }}"
  - "{{ hadoop_tmp_path }}"
  - "{{ hadoop_log_path }}"
  - "{{ hadoop_dfs_path }}"
  - "{{ hadoop_dfs_name_path }}"
  - "{{ hadoop_dfs_data_path }}"

- name: 下载 Hadoop 安装包
  get_url: url={{ hadoop_download_url }} dest=/tmp owner={{ hadoop_user }} group={{ hadoop_group }} mode=644

- name: 解压 Hadoop 安装包
  unarchive:
    # src也可以直接填写一个URL地址直接进行下载解压
    src: "/tmp/hadoop-{{hadoop_version}}.tar.gz"
    copy: no
    dest: "{{ hadoop_work_path }}"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    
- name: 调整 Hadoop 目录所有者
  file: name={{ hadoop_path }} state=directory recurse=yes owner={{ hadoop_user }} group={{ hadoop_group }}
  
- name: 设置 Hadoop 环境变量
  template: src=hadoop_env.sh dest=/etc/profile.d

- name: 生效 Hadoop 环境变量
  shell: "source /etc/profile.d/hadoop_env.sh"

- name: 拷贝 Hadoop 配置文件
  template: src={{ item }} dest={{ hadoop_config_path }} owner={{ hadoop_user }} group={{ hadoop_group }} mode=644
  with_items:
  - core-site.xml
  - hdfs-site.xml
  - mapred-site.xml
  - yarn-site.xml
  - workers

- name: 拷贝 Hadoop 执行文件
  template: src={{ item }} dest={{ hadoop_sbin_path }} owner={{ hadoop_user }} group={{ hadoop_group }} mode=755
  with_items:
  - start-all.sh
  - start-dfs.sh
  - start-yarn.sh
  - stop-all.sh
  - stop-dfs.sh
  - stop-yarn.sh

