- name: 准备 spark 工作目录
  file: name={{ item }} state=directory owner={{ hadoop_user }} group={{ hadoop_group }} mode=0755
  with_items:
  - "{{ spark_work_path }}"
  - "{{ spark_worker_path }}"
  - "{{ spark_log_path }}"
  
- name: 下载 spark 安装包
  get_url: url={{ spark_download_url }} dest=/tmp owner={{ hadoop_user }} group={{ hadoop_group }} mode=644

- name: 解压 spark 安装包
  unarchive:
    # src也可以直接填写一个URL地址直接进行下载解压
    src: "/tmp/spark-{{spark_version}}-bin-hadoop2.7.tgz"
    copy: no
    dest: "{{ spark_work_path }}"
    owner: "{{ hadoop_user }}"
    group: "{{ hadoop_group }}"
    
- name: 调整 spark 目录所有者
  file: name={{ spark_path }} state=directory recurse=yes owner={{ hadoop_user }} group={{ hadoop_group }} 

- name: 设置 spark 环境变量
  template: src=spark_env.sh dest=/etc/profile.d

- name: 生效 spark 环境变量
  shell: "source /etc/profile.d/spark_env.sh"

- name: 拷贝 spark 配置文件
  template: src={{ item }} dest={{ spark_config_path }} owner={{ hadoop_user }} group={{ hadoop_group }} mode=644
  with_items:
  - spark-defaults.conf
  - slaves

- name: 初始化 spark Hdfs 路径
  shell: "hadoop fs -mkdir -p {{ spark_hdfs_path }}"
  become: yes
  become_method: su
  become_user: "{{ hadoop_user }}"

