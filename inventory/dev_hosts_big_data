[hadoop-master]
172.20.32.131 node_name=k8sn131 node_ip="172.20.32.131"
[hadoop-workers]
172.20.32.48 node_name=k8sn48 node_ip="172.20.32.48"
172.20.32.59 node_name=k8sn59 node_ip="172.20.32.59"
[hadoop-cluster:children]
hadoop-master
hadoop-workers
[hive]
172.20.32.131
[hbase-master]
172.20.32.131
[hbase]
172.20.32.131
172.20.32.48
172.20.32.59
[zookeeper]
172.20.32.60
172.20.32.48
172.20.32.59
[spark-master]
172.20.32.131
[spark]
172.20.32.131
172.20.32.48
172.20.32.59
[kylin]
172.20.32.131

[all:vars]

#是否yum更新操作系统内核（不更新则注释参数）
update_yum=true

#时间同步服务器地址（不需要则注释参数）
ntpdate_address="ntp1.aliyun.com"

#是否需要新增操作系统用户（不需要则注释参数）
add_user=true

hadoop_user="hadoop"
hadoop_group="hadoop"

# 指定password参数时，不能直接指定密码，需要先将密码字符串进行加密处理。
# How do I generate crypted passwords for the user module?
# https://docs.ansible.com/ansible/latest/reference_appendices/faq.html#how-do-i-generate-crypted-passwords-for-the-user-module
# 我使用的是macOX生成密码，命令为：
# python3 -m pip install passlib
# python3 -c "from passlib.hash import sha512_crypt; import getpass; print(sha512_crypt.using(rounds=5000).hash(getpass.getpass()))"
# Centos7 下生成密码的命令为：
# echo "hadoop" | openssl passwd -1 -salt $(< /dev/urandom tr -dc '[:alnum:]' | head -c 32) -stdin
hadoop_user_password="$6$FtC9ztboWCq/lbVN$naDU5E.mCkrqy4t3DhtrtP4p1ij7ZWr9a.dkFC7Pdw/GgH3DpfOZtGKOvoHrjL2LHOSBVyeaNdtxKppeze/vK."

hadoop_user_path="/home/hadoop"
hadoop_version="3.1.0"
hadoop_download_url="http://down.belle.cn/package/hadoop/hadoop-{{hadoop_version}}.tar.gz"
#hadoop_download_url="http://apache.claz.org/hadoop/common/hadoop-{hadoop_version}}/hadoop-{hadoop_version}}.tar.gz"
hadoop_work_path="/home/hadoop"
hadoop_path="{{hadoop_work_path}}/hadoop-{{hadoop_version}}"
hadoop_config_path="{{hadoop_path}}/etc/hadoop"
hadoop_sbin_path="{{hadoop_path}}/sbin"
hadoop_log_path="{{hadoop_path}}/hadoop_logs"
hadoop_tmp_path="{{hadoop_path}}/tmp"
hadoop_dfs_path="{{hadoop_path}}/dfs"
hadoop_dfs_name_path="{{hadoop_dfs_path}}/name"
hadoop_dfs_data_path="{{hadoop_dfs_path}}/data"
hadoop_master_ip=172.20.32.131
hadoop_hdfs_port=9000
hadoop_dfs_namenode_http_port=50070
hadoop_dfs_secondary_namenode_http_port=50090
hadoop_yarn_resourcemanager_webapp_port=8088
hadoop_yarn_resourcemanager_tracker_port=8025
hadoop_yarn_resourcemanager_scheduler_port=8030
hadoop_yarn_resourcemanager_port=8040
hadoop_yarn_resourcemanager_admin_port=8141
hadoop_mapreduce_map_memory_mb=1536
hadoop_mapreduce_reduce_memory_mb=3072
hadoop_mapreduce_jobhistory_address_port=10020
hadoop_mapreduce_jobhistory_webapp_address_port=19888

hive_version="2.3.3"
hive_work_path="/home/hadoop"
hive_path="{{hive_work_path}}/apache-hive-{{hive_version}}-bin"
hive_config_path="{{hive_path}}/conf"
hive_lib_path="{{hive_path}}/lib"
hive_logging_operation_log_location="{{hive_work_path}}/hive-{{hive_version}}/tmp/operation_logs"
hive_download_url="http://172.20.32.36/package/hive/apache-hive-{{hive_version}}-bin.tar.gz"
#hive_download_url="http://mirrors.hust.edu.cn/apache/hive/hive-{{hive_version}}/apache-hive-{{hive_version}}-bin.tar.gz"
hive_db_type="mysql"
hive_connection_driver_name="com.mysql.jdbc.Driver"
hive_connection_url="jdbc:mysql://dev.db.belle.cn:3306/db_metastore?createDatabaseIfNotExist=true"
hive_connection_user_name="hive"
hive_connection_password="hive"

hbase_version="1.2.6.1"
hbase_work_path="/home/hadoop"
hbase_path="{{hbase_work_path}}/hbase-{{hbase_version}}"
hbase_config_path="{{hbase_path}}/conf"
hbase_log_path="{{hbase_path}}/logs"
hbase_tmp_path="{{hbase_path}}/tmp"
hbase_logging_operation_log_location="{{hbase_work_path}}/hbase-{{hbase_version}}/tmp/operation_logs"
hbase_download_url="http://172.20.32.36/package/hbase/hbase-{{hbase_version}}-bin.tar.gz"
#hbase_download_url="http://mirrors.hust.edu.cn/apache/hbase/stable/hbase-{{hbase_version}}-bin.tar.gz"
hbase_master_port=60000
hbase_master_info_port=60010
hbase_regionserver_port=60020
hbase_regionserver_info_port=60030
hbase_rest_port=8080
hbase_hdfs_path="/hbase"
hbase_heapsize=5G
hbase_opts="-XX:+UseConcMarkSweepGC -XX:ParallelGCThreads=2"

zk_client_port=2181  

scala_version="2.12.6"
scala_work_path="/home/hadoop"
scala_path="{{scala_work_path}}/scala-{{scala_version}}"
scala_download_url="http://172.20.32.36/package/spark/scala-{{scala_version}}.tgz"
#scala_download_url="https://downloads.lightbend.com/scala/{{scala_version}}/scala-{{scala_version}}.tgz"

spark_version="2.3.1"
spark_work_path="/home/hadoop"
spark_path="{{spark_work_path}}/spark-{{spark_version}}-bin-hadoop2.7"
spark_config_path="{{spark_path}}/conf"
spark_download_url="http://172.20.32.36/package/spark/spark-{{spark_version}}-bin-hadoop2.7.tgz"
#spark_download_url="http://mirrors.hust.edu.cn/apache/spark/spark-{{spark_version}}/spark-{{spark_version}}-bin-hadoop2.7.tgz"
spark_worker_path="{{spark_path}}/spark/worker"
spark_log_path="{{spark_path}}/spark/logs"
spark_hdfs_path="hdfs://{{ hadoop_master_ip }}:{{ hadoop_hdfs_port }}/spark/history_log"
spark_master_port=17077
spark_history_ui_port=17777
spark_web_port=18080

kylin_version="2.3.2"
kylin_work_path="/home/hadoop"
kylin_path="{{kylin_work_path}}/apache-kylin-{{kylin_version}}-bin-{{kylin_env}}"
kylin_config_path="{{kylin_path}}/conf"
kylin_env="hbase1x"
kylin_download_url="http://172.20.32.36/package/kylin/apache-kylin-{{kylin_version}}-bin-{{kylin_env}}.tar.gz"
#kylin_download_url="http://mirrors.hust.edu.cn/apache/kylin/apache-kylin-{{kylin_version}}/apache-kylin-{{kylin_version}}-bin-{{kylin_env}}.tar.gz"
